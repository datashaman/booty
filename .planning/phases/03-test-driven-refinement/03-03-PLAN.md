---
phase: 03-test-driven-refinement
plan: 03
type: execute
wave: 2
depends_on: ["03-01", "03-02"]
files_modified:
  - src/booty/llm/prompts.py
  - src/booty/code_gen/refiner.py
  - src/booty/code_gen/generator.py
  - src/booty/main.py
autonomous: true

must_haves:
  truths:
    - "On test failure, LLM receives error output and regenerates only affected files"
    - "System retries up to max_retries times, each iteration sees only latest failure"
    - "When tests pass, PR is created as ready for review (not draft)"
    - "On permanent failure after max retries, system comments on issue and opens draft PR"
    - "Transient API errors retry with exponential backoff via tenacity"
    - "Missing .booty.yml causes job to fail with clear error message"
  artifacts:
    - path: "src/booty/llm/prompts.py"
      provides: "Refinement prompt for targeted code regeneration"
      contains: "def regenerate_code"
    - path: "src/booty/code_gen/refiner.py"
      provides: "Refinement loop: test -> analyze -> regenerate -> repeat"
      contains: "async def refine_until_tests_pass"
    - path: "src/booty/code_gen/generator.py"
      provides: "Updated pipeline with test-driven refinement integrated"
      contains: "refine_until_tests_pass"
    - path: "src/booty/main.py"
      provides: "Updated process_job with failure handling"
      contains: "post_failure_comment"
  key_links:
    - from: "src/booty/code_gen/refiner.py"
      to: "src/booty/test_runner/executor.py"
      via: "execute_tests call in refinement loop"
      pattern: "execute_tests"
    - from: "src/booty/code_gen/refiner.py"
      to: "src/booty/llm/prompts.py"
      via: "regenerate_code call for targeted file regeneration"
      pattern: "regenerate_code"
    - from: "src/booty/code_gen/refiner.py"
      to: "src/booty/test_runner/parser.py"
      via: "extract_files_from_output for targeted regeneration"
      pattern: "extract_files_from_output"
    - from: "src/booty/code_gen/generator.py"
      to: "src/booty/code_gen/refiner.py"
      via: "refine_until_tests_pass after initial code generation"
      pattern: "refine_until_tests_pass"
    - from: "src/booty/main.py"
      to: "src/booty/github/comments.py"
      via: "post_failure_comment on permanent failure"
      pattern: "post_failure_comment"
---

<objective>
Wire the refinement loop: add LLM refinement prompt, create the test-refine iteration loop, integrate into the pipeline, and add failure handling to process_job.

Purpose: This is the core of Phase 3 — connecting test execution to LLM regeneration in a loop, with proper success/failure outcomes.
Output: Working end-to-end pipeline where generated code is tested, failures are fed back to LLM, and the system retries until tests pass or max retries exhausted.
</objective>

<execution_context>
@/Users/marlinf/.claude/get-shit-done/workflows/execute-plan.md
@/Users/marlinf/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-test-driven-refinement/03-RESEARCH.md
@.planning/phases/03-test-driven-refinement/03-01-SUMMARY.md
@.planning/phases/03-test-driven-refinement/03-02-SUMMARY.md

@src/booty/code_gen/generator.py
@src/booty/llm/prompts.py
@src/booty/llm/models.py
@src/booty/main.py
@src/booty/config.py
@src/booty/test_runner/config.py
@src/booty/test_runner/executor.py
@src/booty/test_runner/parser.py
@src/booty/github/pulls.py
@src/booty/github/comments.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add refinement prompt and create refiner module</name>
  <files>src/booty/llm/prompts.py, src/booty/code_gen/refiner.py</files>
  <action>
    1. Add to `src/booty/llm/prompts.py` a new function `regenerate_code`:
       - Decorated with `@prompt` (magentic)
       - Takes: `task_description: str`, `file_contents_formatted: str`, `error_output: str`, `failed_files: str`, `issue_title: str`, `issue_body: str`, `model: AnthropicChatModel`
       - Returns: `CodeGenerationPlan`
       - Prompt content (key points):
         - System role: "You are a code generation assistant fixing failing tests"
         - UNTRUSTED delimiters around issue content (same pattern as existing prompts)
         - Includes current file contents, error output, and which files were identified in the failure
         - Instructions: "Analyze the test error output. Regenerate ONLY the files that need fixing. Preserve files that work correctly. Return COMPLETE file contents (not diffs)."
         - Emphasize: focus on the specific error, don't over-modify working code
       - Also add a wrapper `regenerate_code_changes(task_description, file_contents, error_output, failed_files, issue_title, issue_body, model)` that formats file_contents using `_format_file_contents()` before calling the prompt function (same pattern as `generate_code_changes`).
       - Wrap the inner prompt call with tenacity `@retry` decorator for transient API errors:
         - `retry=retry_if_exception_type((RateLimitError, APITimeoutError, asyncio.TimeoutError))`
         - `wait=wait_exponential(multiplier=1, min=4, max=60)`
         - `stop=stop_after_attempt(5)`
         - `reraise=True`
         - Import RateLimitError and APITimeoutError from `anthropic`
         - Import tenacity decorators at top of file

    2. Create `src/booty/code_gen/refiner.py` with:
       - `async def refine_until_tests_pass(workspace_path: Path, config: BootyConfig, current_changes: list[FileChange], task_description: str, issue_title: str, issue_body: str, model: AnthropicChatModel, settings: Settings) -> tuple[bool, list[FileChange], str | None]`:
         - Returns `(tests_passed, final_changes, error_message_or_none)`
         - Loop from attempt 1 to `config.max_retries`:
           a. Call `execute_tests(config.test_command, config.timeout, workspace_path)` to run tests
           b. If `result.exit_code == 0`: log success, return `(True, current_changes, None)`
           c. If timed_out or exit_code != 0 on last attempt: extract error summary, return `(False, current_changes, error_summary)`
           d. Otherwise (failure, not last attempt):
              - Extract error summary from test output via `extract_error_summary(result.stderr, result.stdout)`
              - Extract failing files via `extract_files_from_output(result.stderr + "\n" + result.stdout, workspace_path)`
              - If no files identified from traceback, use all originally modified file paths as fallback
              - Build current file_contents dict by reading the current state of all changed files from workspace
              - Call `regenerate_code_changes(task_description, file_contents, error_summary, ", ".join(failed_files), issue_title, issue_body, model)`
              - Validate regenerated code (reuse `validate_generated_code` from code_gen.validator)
              - Apply new changes to workspace (write files, handle create/modify/delete)
              - Update `current_changes` to the new plan's changes
              - Log each attempt: attempt number, exit code, files regenerated
         - Use structlog logger from booty.logging
         - IMPORTANT: Each iteration only sees latest code + latest failure (no cumulative history)
  </action>
  <verify>
    - `python -c "from booty.llm.prompts import regenerate_code_changes; print('OK')"` succeeds
    - `python -c "from booty.code_gen.refiner import refine_until_tests_pass; print('OK')"` succeeds
    - `python -c "import inspect; from booty.code_gen.refiner import refine_until_tests_pass; assert inspect.iscoroutinefunction(refine_until_tests_pass); print('OK')"` succeeds
  </verify>
  <done>Refinement prompt feeds error output to LLM for targeted regeneration. Refiner loops test->analyze->regenerate with last-attempt-only context, returns success/failure with final changes.</done>
</task>

<task type="auto">
  <name>Task 2: Integrate refinement into pipeline and add failure handling</name>
  <files>src/booty/code_gen/generator.py, src/booty/main.py</files>
  <action>
    1. Modify `src/booty/code_gen/generator.py` — `process_issue_to_pr()`:
       - After Step 9 (apply changes to workspace) and before Step 10 (commit), insert the refinement loop:
         a. Load .booty.yml config: `config = load_booty_config(workspace_path)` — wrap in try/except FileNotFoundError to raise ValueError with clear message that .booty.yml is required
         b. Call `refine_until_tests_pass(workspace_path, config, plan.changes, analysis.task_description, issue_title, issue_body, model, settings)`
         c. Store result: `tests_passed, final_changes, error_message = ...`
         d. If changes were regenerated (final_changes differ from plan.changes), re-apply them to workspace (same apply logic as Step 9)
       - Modify Step 12 (create PR):
         - Pass `draft=not tests_passed` to `create_pull_request()`
         - If tests failed, append error context to PR body (add a "## Test Failures" section)
       - Return a tuple or modify return to include `tests_passed` and `error_message` so the caller can handle failure notification
       - Change return type to `tuple[int, bool, str | None]` — `(pr_number, tests_passed, error_message)`
       - Add imports: `from booty.test_runner.config import load_booty_config`, `from booty.code_gen.refiner import refine_until_tests_pass`

    2. Modify `src/booty/main.py` — `process_job()`:
       - Update to unpack the new return type: `pr_number, tests_passed, error_message = await process_issue_to_pr(...)`
       - If `not tests_passed`:
         - Call `post_failure_comment(settings.GITHUB_TOKEN, settings.TARGET_REPO_URL, job.issue_number, error_message, config.max_retries, config.max_retries)`
         - Log `job_completed_with_failures`
         - Note: need to access config.max_retries — either pass it back from generator or load config in main.py. Simplest: have generator return attempt count. Alternative: just log a fixed message. Choose simplest: pass `max_retries` in the error_message string from generator, or return it. Best approach: have `process_issue_to_pr` return enough info. Change return to `tuple[int, bool, str | None]` where error_message includes attempt count context.
         - For the comment, extract attempt info from the error message OR simply load .booty.yml config in main.py too. Actually, simplest clean approach: have process_issue_to_pr raise a custom exception on failure that includes all context, and main.py catches it. But that changes error flow. Cleanest: just have the generator log and the main.py call post_failure_comment with the error_message as-is. The error_message from refiner already has context.
         - Implementation: In main.py, if not tests_passed, call `post_failure_comment(settings.GITHUB_TOKEN, settings.TARGET_REPO_URL, job.issue_number, error_message or "Unknown error", 0, 0)`. The refiner should format error_message to include attempt info like "After 3/3 attempts: {error_summary}".
       - If tests_passed: log `job_completed_successfully`
       - Add import: `from booty.github.comments import post_failure_comment`

    IMPORTANT: The existing exception handler in generator.py's try/except should remain — it handles unexpected errors. The new failure path is for expected test failures (code generated but tests don't pass).
  </action>
  <verify>
    - `python -c "from booty.code_gen.generator import process_issue_to_pr; print('OK')"` succeeds
    - `python -c "from booty.main import process_job; print('OK')"` succeeds
    - Grep for `refine_until_tests_pass` in generator.py confirms integration
    - Grep for `post_failure_comment` in main.py confirms failure handling
    - Grep for `draft` in generator.py confirms draft PR support
  </verify>
  <done>Pipeline runs tests after code generation, retries on failure, creates ready PR on success, creates draft PR + issue comment on permanent failure. Missing .booty.yml fails the job.</done>
</task>

</tasks>

<verification>
- Full import chain works: `python -c "from booty.main import process_job; from booty.code_gen.generator import process_issue_to_pr; from booty.code_gen.refiner import refine_until_tests_pass; print('OK')"`
- generator.py calls `load_booty_config` and `refine_until_tests_pass`
- generator.py passes `draft=` to `create_pull_request`
- main.py calls `post_failure_comment` on test failure
- refiner.py calls `execute_tests`, `extract_error_summary`, `extract_files_from_output`, `regenerate_code_changes`
- No circular imports between modules
</verification>

<success_criteria>
- Test failure triggers LLM regeneration with error context (targeted, not full regeneration)
- Refinement loop runs up to max_retries from .booty.yml config
- Each retry sees only latest failure output (no cumulative history)
- Passing tests produce non-draft PR
- Failed tests after max retries produce draft PR + issue comment with error details
- Transient API errors (rate limit, timeout) retry with exponential backoff
- Missing .booty.yml fails the job explicitly
</success_criteria>

<output>
After completion, create `.planning/phases/03-test-driven-refinement/03-03-SUMMARY.md`
</output>
