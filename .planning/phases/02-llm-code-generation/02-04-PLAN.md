---
phase: 02-llm-code-generation
plan: 04
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/booty/llm/prompts.py
  - src/booty/llm/token_budget.py
autonomous: true

must_haves:
  truths:
    - "Issue content is analyzed by LLM and returns structured IssueAnalysis"
    - "Code is generated as complete file contents (not diffs) with FileChange models"
    - "Token budget is checked BEFORE generation to prevent context overflow"
    - "Issue content is sandboxed in prompts (untrusted content delimiters)"
  artifacts:
    - path: "src/booty/llm/prompts.py"
      provides: "Magentic @prompt functions for issue analysis and code generation"
      exports: ["analyze_issue", "generate_code_changes"]
      min_lines: 50
    - path: "src/booty/llm/token_budget.py"
      provides: "Token counting and budget management"
      exports: ["TokenBudget", "estimate_tokens"]
      min_lines: 30
  key_links:
    - from: "src/booty/llm/prompts.py"
      to: "magentic"
      via: "@prompt decorator with AnthropicChatModel"
      pattern: "@prompt"
    - from: "src/booty/llm/prompts.py"
      to: "src/booty/llm/models.py"
      via: "Return types IssueAnalysis, CodeGenerationPlan"
      pattern: "IssueAnalysis|CodeGenerationPlan"
    - from: "src/booty/llm/token_budget.py"
      to: "anthropic"
      via: "messages.count_tokens API"
      pattern: "count_tokens"
---

<objective>
Build the LLM interaction layer: magentic prompts for issue analysis and code generation, plus token budget tracking to prevent context overflow.

Purpose: REQ-07 (issue analysis), REQ-08 (code generation), REQ-09 (context budget management).
Output: Prompt functions that analyze issues and generate code, with token budget enforcement.
</objective>

<execution_context>
@/Users/marlinf/.claude/get-shit-done/workflows/execute-plan.md
@/Users/marlinf/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-llm-code-generation/02-RESEARCH.md
@.planning/phases/02-llm-code-generation/02-CONTEXT.md
@.planning/phases/02-llm-code-generation/02-01-SUMMARY.md
@src/booty/config.py
@src/booty/llm/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Token budget tracking</name>
  <files>src/booty/llm/token_budget.py</files>
  <action>
  Create src/booty/llm/token_budget.py with:

  **class TokenBudget:**
  - __init__(self, model: str, max_context_tokens: int, max_output_tokens: int)
  - Store model, max_context_tokens, max_output_tokens
  - Create anthropic.Anthropic() client (uses ANTHROPIC_API_KEY env var)

  **estimate_tokens(self, system_prompt: str, user_content: str) -> int**
  - Call self.client.messages.count_tokens(model=self.model, system=system_prompt, messages=[{"role": "user", "content": user_content}])
  - Return response.input_tokens

  **check_budget(self, system_prompt: str, user_content: str) -> dict**
  - Call estimate_tokens
  - Calculate remaining = max_context_tokens - input_tokens - max_output_tokens (reserve space for output)
  - Return {"input_tokens": int, "output_reserved": max_output_tokens, "remaining": int, "fits": bool, "overflow_by": int}
  - fits = remaining >= 0, overflow_by = max(0, -remaining)

  **select_files_within_budget(self, system_prompt: str, base_content: str, file_contents: dict[str, str], max_context: int) -> dict[str, str]**
  - Start with base_content token count
  - Add files one at a time (sorted by path for determinism)
  - For each file, estimate tokens for base_content + accumulated files + new file
  - Stop adding when next file would exceed budget
  - Return dict of selected files (path -> content)
  - Log which files were included and which were dropped

  Use booty.logging.get_logger for structured logging.
  Import anthropic at top of file.
  </action>
  <verify>
  Run: `python -c "from booty.llm.token_budget import TokenBudget; print('OK')"`
  (Full integration test requires ANTHROPIC_API_KEY, so just verify import and class structure)
  </verify>
  <done>TokenBudget class can estimate tokens via Anthropic API, check if content fits within budget, and select files that fit within token limits.</done>
</task>

<task type="auto">
  <name>Task 2: Magentic LLM prompts for issue analysis and code generation</name>
  <files>src/booty/llm/prompts.py</files>
  <action>
  Create src/booty/llm/prompts.py with magentic @prompt functions:

  1. **Helper to build model instance:**
  ```python
  def get_llm_model(model: str, temperature: float, max_tokens: int) -> AnthropicChatModel:
      return AnthropicChatModel(model, temperature=temperature, max_tokens=max_tokens)
  ```

  2. **analyze_issue function using @prompt:**
  - Takes: issue_title: str, issue_body: str, repo_file_list: str (newline-separated file paths in repo)
  - Returns: IssueAnalysis (from booty.llm.models)
  - System prompt establishes the agent role as code generation assistant
  - Issue content placed in UNTRUSTED delimiters per CONTEXT.md decisions:
    ```
    === BEGIN UNTRUSTED ISSUE CONTENT ===
    Title: {issue_title}
    Body: {issue_body}
    === END UNTRUSTED ISSUE CONTENT ===
    ```
  - Prompt instructs LLM to analyze issue and identify files to change, create, delete
  - Provide the repo file list so LLM knows what exists
  - Use max_retries=3 for validation failures
  - Accept model as parameter so caller can configure

  3. **generate_code_changes function using @prompt:**
  - Takes: analysis_summary: str, file_contents: dict[str, str] (path -> current content), issue_title: str, issue_body: str
  - Returns: CodeGenerationPlan (from booty.llm.models)
  - Issue content again in UNTRUSTED delimiters
  - Prompt instructs:
    - Generate COMPLETE file contents (not diffs)
    - Include all files that need changes
    - For new files, provide full content
    - For deletions, set operation="delete" and content=""
    - Follow existing code style and conventions visible in provided files
  - Use max_retries=3
  - Accept model as parameter

  IMPORTANT: Both functions should accept the model as a parameter (not create it internally) so the orchestrator can configure model/temperature/max_tokens from settings. Provide a convenience wrapper or document that the caller builds the model.

  IMPORTANT: Use the format from RESEARCH.md Pattern 1. The @prompt decorator goes on the function, the function body is just `...` (ellipsis).

  Import: from magentic import prompt
  Import: from magentic.chat_model.anthropic_chat_model import AnthropicChatModel
  Import: from booty.llm.models import IssueAnalysis, CodeGenerationPlan
  </action>
  <verify>
  Run: `python -c "from booty.llm.prompts import analyze_issue, generate_code_changes, get_llm_model; print('OK')"`
  (Full test requires ANTHROPIC_API_KEY and actual LLM calls)
  </verify>
  <done>analyze_issue returns IssueAnalysis with structured issue understanding. generate_code_changes returns CodeGenerationPlan with full file contents. Both use sandboxed prompts for untrusted issue content.</done>
</task>

</tasks>

<verification>
- [ ] TokenBudget importable and instantiable
- [ ] TokenBudget.check_budget returns correct dict structure
- [ ] analyze_issue decorated with @prompt, returns IssueAnalysis
- [ ] generate_code_changes decorated with @prompt, returns CodeGenerationPlan
- [ ] Both prompts have UNTRUSTED content delimiters
- [ ] Both prompts use max_retries=3
- [ ] Full file generation (not diffs) instructed in code gen prompt
</verification>

<success_criteria>
LLM interaction layer complete. Issue analysis extracts structured understanding. Code generation produces complete file contents. Token budget prevents context overflow. All prompts sandbox untrusted issue content.
</success_criteria>

<output>
After completion, create `.planning/phases/02-llm-code-generation/02-04-SUMMARY.md`
</output>
