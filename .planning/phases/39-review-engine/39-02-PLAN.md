---
phase: 39-review-engine
plan: 02
type: execute
wave: 2
depends_on: ["39-01"]
files_modified:
  - src/booty/reviewer/runner.py
  - src/booty/reviewer/engine.py
  - src/booty/github/comments.py
  - tests/test_reviewer_runner.py
autonomous: true

must_haves:
  truths:
    - "Runner fetches PR diff via repo.compare(base_sha, head_sha)"
    - "Runner calls run_review and gets ReviewResult"
    - "Check conclusion: success for APPROVED/APPROVED_WITH_SUGGESTIONS, failure for BLOCKED"
    - "Check output title: REV-05 — 'Reviewer approved', 'Reviewer approved with suggestions', or 'Reviewer blocked'"
    - "PR comment posted with format_reviewer_comment body and <!-- booty-reviewer --> marker"
  artifacts:
    - path: src/booty/reviewer/runner.py
      provides: process_reviewer_job with LLM review integration
    - path: src/booty/reviewer/engine.py
      provides: format_reviewer_comment(result) -> str
  key_links:
    - from: runner.process_reviewer_job
      to: engine.run_review
      via: call with diff and config.block_on
    - from: runner
      to: post_reviewer_comment
      via: format_reviewer_comment then post
---

<objective>
Integrate Review Engine into runner: fetch diff, call run_review, format comment, update check run and post PR comment.

Purpose: REV-06, REV-07, REV-08, REV-10 — Runner uses engine; check conclusion and title per REV-05; single updatable comment with CONTEXT.md format.
Output: runner.py with full LLM flow; format_reviewer_comment; tests.
</objective>

<execution_context>
@/Users/marlinf/.claude/get-shit-done/workflows/execute-plan.md
@/Users/marlinf/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/39-review-engine/39-CONTEXT.md
@.planning/phases/39-review-engine/39-RESEARCH.md
@src/booty/reviewer/runner.py
@src/booty/github/comments.py
@src/booty/github/checks.py
@src/booty/security/runner.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: format_reviewer_comment and diff fetching</name>
  <files>src/booty/reviewer/engine.py, src/booty/reviewer/runner.py</files>
  <action>
1. **Add format_reviewer_comment(result: ReviewResult) -> str in engine.py** (or comments.py — CONTEXT says single formatter). Per CONTEXT.md comment format:
   - Header: "Reviewer: APPROVED | APPROVED_WITH_SUGGESTIONS | BLOCKED"
   - Rationale line (if BLOCKED): "Blocking: {blocking_categories}"
   - Sections in order: Overengineering, Architectural drift, Tests, Duplication, Maintainability, Naming/API
   - Each section: "### {category}\nStatus: PASS | WARN | FAIL\n" + up to 3 bullets (path, what's wrong, improvement). Overflow: "(+N more)"
   - Wrap in `<!-- booty-reviewer -->` ... `<!-- /booty-reviewer -->` (marker for find-and-edit)
   - Keep review-level, not lint-level; no line-by-line annotations.

2. **In runner.py, before edit_check_run(completed):**
   - base_sha = job.payload.get("pull_request", {}).get("base", {}).get("sha", "") or ""
   - If not base_sha, use head_sha as base (empty diff)
   - compare = repo.compare(base_sha, job.head_sha)
   - Build unified diff: concatenate f.patch for each compare.files (f.patch can be None for binaries — skip or use placeholder)
   - Build pr_meta: title, body from payload; file_list with path and tests/ prefix
   - Truncate diff to ~80k chars if needed
   - result = run_review(diff, pr_meta, config.block_on) — use asyncio.to_thread if run_review is sync
  </action>
  <verify>python -c "from booty.reviewer.engine import format_reviewer_comment, run_review; print('OK')"</verify>
  <done>format_reviewer_comment exists; runner fetches diff and calls run_review</done>
</task>

<task type="auto">
  <name>Task 2: Check run conclusion and comment posting</name>
  <files>src/booty/reviewer/runner.py</files>
  <action>
Replace stub edit_check_run(completed, success) with:

1. **Check conclusion per REV-05:**
   - APPROVED → conclusion="success", output title="Reviewer approved"
   - APPROVED_WITH_SUGGESTIONS → conclusion="success", output title="Reviewer approved with suggestions"
   - BLOCKED → conclusion="failure", output title="Reviewer blocked"
   - output summary = short rationale or "See PR comment for details"

2. **Post comment:**
   - body = format_reviewer_comment(result)
   - post_reviewer_comment(settings.GITHUB_TOKEN, repo_url, job.pr_number, body)
   - repo_url: construct from owner/repo_name or job.repo_url

3. **Error handling:** Wrap run_review in try/except. On LLM/infra failure: Phase 41 adds fail-open. For Phase 39, re-raise or log and set conclusion="failure" with "Reviewer error" — keep flow simple; Phase 41 will add reviewer_fail_open.
  </action>
  <verify>python -c "from booty.reviewer.runner import process_reviewer_job; print('OK')"</verify>
  <done>Check conclusion and title correct; comment posted with marker</done>
</task>

<task type="auto">
  <name>Task 3: Tests for runner integration</name>
  <files>tests/test_reviewer_runner.py</files>
  <action>
Create tests/test_reviewer_runner.py:

1. **Mock run_review** to return fixed ReviewResult (APPROVED, BLOCKED, APPROVED_WITH_SUGGESTIONS cases)
2. **Mock repo.compare** to return Compare with sample files/patch
3. **Mock edit_check_run, post_reviewer_comment**
4. Test: process_reviewer_job with APPROVED result → edit_check_run called with conclusion="success", title="Reviewer approved"
5. Test: process_reviewer_job with BLOCKED result → edit_check_run conclusion="failure", title="Reviewer blocked"; post_reviewer_comment called with body containing <!-- booty-reviewer -->
6. Test: format_reviewer_comment produces valid structure (sections, marker)

Use pytest-asyncio; process_reviewer_job is async.
  </action>
  <verify>pytest tests/test_reviewer_runner.py -v</verify>
  <done>Runner integration tests pass</done>
</task>

</tasks>

<verification>
- REV-05 check titles covered
- REV-10 single comment with marker
- REV-06, REV-07, REV-08 outcomes flow to check and comment
</verification>

<success_criteria>
- process_reviewer_job performs full LLM review
- Check conclusion and title match decision
- PR comment formatted per CONTEXT
- Tests pass
</success_criteria>

<output>
After completion, create .planning/phases/39-review-engine/39-02-SUMMARY.md
</output>
