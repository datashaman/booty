---
phase: 39-review-engine
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/booty/reviewer/schema.py
  - src/booty/reviewer/prompts.py
  - src/booty/reviewer/engine.py
  - src/booty/reviewer/__init__.py
  - tests/test_reviewer_engine.py
autonomous: true

must_haves:
  truths:
    - "run_review returns ReviewResult with decision and 6 category grades"
    - "block_on mapping determines which category FAIL blocks promotion"
    - "Decision logic: any enabled blocker FAIL → BLOCKED; else any WARN/FAIL → APPROVED_WITH_SUGGESTIONS; else APPROVED"
    - "LLM prompt receives unified diff and PR metadata, evaluates quality only (no lint/tests reruns)"
  artifacts:
    - path: src/booty/reviewer/schema.py
      provides: ReviewResult, CategoryResult, Finding, ReviewDecision
    - path: src/booty/reviewer/engine.py
      provides: run_review(diff, pr_meta, block_on) -> ReviewResult
    - path: src/booty/reviewer/prompts.py
      provides: Magentic prompt producing structured ReviewResult
  key_links:
    - from: engine.run_review
      to: prompts
      via: Magentic LLM call
---

<objective>
Create Review Engine: Pydantic output schema, Magentic LLM prompt, block_on mapping, and decision logic.

Purpose: REV-06, REV-07, REV-08, REV-11 — LLM evaluates 6 categories (maintainability, overengineering, duplication, tests, naming/API, architectural drift); produces APPROVED/APPROVED_WITH_SUGGESTIONS/BLOCKED; block_on config controls which categories block.
Output: schema.py, prompts.py, engine.py; run_review() returns ReviewResult; tests for decision logic.
</objective>

<execution_context>
@/Users/marlinf/.claude/get-shit-done/workflows/execute-plan.md
@/Users/marlinf/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/39-review-engine/39-CONTEXT.md
@.planning/phases/39-review-engine/39-RESEARCH.md
@src/booty/planner/generation.py
@src/booty/llm/prompts.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Review schema and output models</name>
  <files>src/booty/reviewer/schema.py</files>
  <action>
Create src/booty/reviewer/schema.py with Pydantic models per CONTEXT.md:

1. **ReviewDecision** — Literal["APPROVED", "APPROVED_WITH_SUGGESTIONS", "BLOCKED"] (LLM outputs per-category; engine computes final)

2. **Finding** — summary: str, detail: str, paths: list[str], line_refs: list[str] | None = None, suggestion: str | None = None

3. **CategoryResult** — category: str (one of Overengineering, Architectural drift, Tests, Duplication, Maintainability, Naming/API); grade: Literal["PASS", "WARN", "FAIL"]; findings: list[Finding]; confidence: Literal["low", "med", "high"] = "med"

4. **ReviewResult** — decision: ReviewDecision; categories: list[CategoryResult]; blocking_categories: list[str] = [] (for comment rationale when BLOCKED)

The LLM prompt will return raw category grades; engine applies block_on logic to compute final decision. ReviewResult is the engine output consumed by runner.
  </action>
  <verify>python -c "from booty.reviewer.schema import ReviewResult, CategoryResult, Finding, ReviewDecision; r=ReviewResult(decision='APPROVED', categories=[]); print(r.decision)"</verify>
  <done>Schema models importable; ReviewResult has decision and categories</done>
</task>

<task type="auto">
  <name>Task 2: Magentic prompt and run_review</name>
  <files>src/booty/reviewer/prompts.py, src/booty/reviewer/engine.py</files>
  <action>
Create reviewer/prompts.py with Magentic @prompt:

1. **Prompt design** (per CONTEXT.md):
   - Input: unified_diff (str), pr_title (str), pr_body (str), base_sha (str), head_sha (str), file_list (str with path and type per file; mark tests/ prefix)
   - Instruct: quality only — maintainability, overengineering, duplication, test quality, naming/API, architectural regression. No lint, format, or style. No re-running tests.
   - Output: 6 CategoryResult objects (one per category). Each: grade (PASS/WARN/FAIL), findings (up to 3 per category; summary, detail, paths, suggestion), confidence.
   - Categories in order: Overengineering, Architectural drift, Tests, Duplication, Maintainability, Naming/API

2. **Engine (engine.py)**:
   - BLOCK_ON_TO_CATEGORY = {"overengineering": "Overengineering", "poor_tests": "Tests", "duplication": "Duplication", "architectural_regression": "Architectural drift"}
   - run_review(diff: str, pr_meta: dict, block_on: list[str]) -> ReviewResult:
     - Call Magentic prompt (sync or asyncio.to_thread for async runner)
     - Map block_on to category names; only these can block
     - If any enabled blocker category has grade FAIL → decision=BLOCKED, blocking_categories=[...]
     - Elif any category has WARN or FAIL → decision=APPROVED_WITH_SUGGESTIONS
     - Else → decision=APPROVED
     - block_on empty → max decision is APPROVED_WITH_SUGGESTIONS (never BLOCKED)
   - Truncate diff if needed (e.g. 80k chars) to avoid token overflow
  </action>
  <verify>python -c "from booty.reviewer.engine import run_review; print('OK')"</verify>
  <done>run_review exists; block_on mapping and decision logic implemented</done>
</task>

<task type="auto">
  <name>Task 3: Tests for decision logic</name>
  <files>tests/test_reviewer_engine.py</files>
  <action>
Create tests/test_reviewer_engine.py. Mock the LLM (or use a minimal prompt that returns known output) to test:

1. test_block_on_mapping — block_on ["overengineering"] + Overengineering FAIL → BLOCKED
2. test_block_on_empty_never_blocks — block_on [] + any FAIL → APPROVED_WITH_SUGGESTIONS max
3. test_maintainability_never_blocks — Maintainability FAIL, block_on empty → APPROVED_WITH_SUGGESTIONS
4. test_all_pass_approved — all PASS → APPROVED
5. test_warn_approval_with_suggestions — any WARN, block_on has that category but grade WARN → APPROVED_WITH_SUGGESTIONS

Use a test double: patch the Magentic call to return fixed CategoryResult list, then assert run_review output decision.
  </action>
  <verify>pytest tests/test_reviewer_engine.py -v</verify>
  <done>Decision logic tests pass</done>
</task>

</tasks>

<verification>
- run_review imports and block_on logic works
- Schema validates LLM-compatible structure
- REV-06, REV-07, REV-08, REV-11 covered by engine
</verification>

<success_criteria>
- schema.py, prompts.py, engine.py exist
- run_review(diff, pr_meta, block_on) returns ReviewResult
- block_on mapping and decision logic correct
- Tests pass
</success_criteria>

<output>
After completion, create .planning/phases/39-review-engine/39-01-SUMMARY.md
</output>
