---
phase: 01-webhook-to-workspace-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/booty/jobs.py
  - src/booty/repositories.py
  - src/booty/webhooks.py
  - src/booty/main.py
autonomous: true

must_haves:
  truths:
    - "Webhook endpoint receives GitHub issue labeled events and returns 200 OK within 2 seconds"
    - "Same webhook event delivered twice produces no duplicate jobs"
    - "Jobs process asynchronously after webhook returns"
    - "Each job gets a fresh clone in an isolated temporary directory"
    - "All operations produce JSON logs with correlation IDs"
  artifacts:
    - path: "src/booty/jobs.py"
      provides: "Job model, JobState enum, JobQueue with async workers"
      exports: ["Job", "JobState", "JobQueue"]
    - path: "src/booty/repositories.py"
      provides: "Repository cloning and workspace management"
      exports: ["clone_repository", "prepare_workspace"]
    - path: "src/booty/webhooks.py"
      provides: "FastAPI webhook route with HMAC verification"
      exports: ["router"]
    - path: "src/booty/main.py"
      provides: "FastAPI app with middleware and lifecycle"
      exports: ["app"]
  key_links:
    - from: "src/booty/webhooks.py"
      to: "src/booty/jobs.py"
      via: "enqueue job after signature verification"
      pattern: "job_queue\\.enqueue"
    - from: "src/booty/jobs.py"
      to: "src/booty/repositories.py"
      via: "worker calls clone_repository during job processing"
      pattern: "clone_repository|prepare_workspace"
    - from: "src/booty/webhooks.py"
      to: "src/booty/config.py"
      via: "reads WEBHOOK_SECRET and TRIGGER_LABEL from settings"
      pattern: "get_settings\\(\\)"
    - from: "src/booty/main.py"
      to: "src/booty/logging.py"
      via: "configure_logging called on startup"
      pattern: "configure_logging"
---

<objective>
Build the webhook handler, async job queue, and repository manager that form the core webhook-to-workspace pipeline.

Purpose: This is the main deliverable of Phase 1 — receiving webhook events and preparing isolated workspaces.
Output: Working FastAPI app that accepts GitHub webhooks, deduplicates events, queues jobs, and clones repos into temp directories.
</objective>

<execution_context>
@/Users/marlinf/.claude/get-shit-done/workflows/execute-plan.md
@/Users/marlinf/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-webhook-to-workspace-pipeline/01-RESEARCH.md
@.planning/phases/01-webhook-to-workspace-pipeline/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Job queue with state tracking and idempotency</name>
  <files>src/booty/jobs.py</files>
  <action>
Create `src/booty/jobs.py` with:

**JobState enum:** QUEUED, RUNNING, COMPLETED, FAILED

**Job dataclass:**
- `job_id: str` (constructed from issue number + delivery ID)
- `issue_url: str`
- `issue_number: int`
- `payload: dict`
- `state: JobState = JobState.QUEUED`
- `created_at: datetime` (auto-set)
- `error: str | None = None`

**JobQueue class:**
- `__init__(self, maxsize: int = 100)` — creates `asyncio.Queue(maxsize=maxsize)`, `jobs: dict[str, Job]`, and `processed_deliveries: set()` for idempotency
- `is_duplicate(self, delivery_id: str) -> bool` — checks if delivery_id already in processed_deliveries set. Cap set at 10,000 entries (pop oldest if exceeded using a deque for ordering).
- `mark_processed(self, delivery_id: str)` — adds to processed_deliveries
- `async enqueue(self, job: Job) -> bool` — returns False if duplicate (job_id already in self.jobs). Stores job, puts on queue. Uses `asyncio.wait_for(self.queue.put(job), timeout=1.0)` to prevent blocking. Logs enqueue event.
- `async worker(self, worker_id: int, process_fn)` — infinite loop: get job from queue, set RUNNING, call `await process_fn(job)`, set COMPLETED or FAILED. Log all state transitions with job_id and worker_id. Catch all exceptions to prevent worker death.
- `async start_workers(self, num_workers: int, process_fn)` — creates N worker tasks via `asyncio.create_task`
- `async shutdown(self)` — cancel all worker tasks gracefully
- `get_job(self, job_id: str) -> Job | None` — lookup by ID

Use structlog via `from booty.logging import get_logger`. Log with job_id, worker_id, issue_number as bound context.

IMPORTANT: Python imports must be at the top of the file.
  </action>
  <verify>
Run:
```
python -c "
import asyncio
from booty.jobs import Job, JobState, JobQueue

async def test():
    q = JobQueue(maxsize=10)
    job = Job(job_id='test-1', issue_url='http://test', issue_number=1, payload={})
    assert job.state == JobState.QUEUED

    # Test enqueue
    result = await q.enqueue(job)
    assert result == True

    # Test duplicate
    result = await q.enqueue(job)
    assert result == False

    # Test delivery dedup
    q.mark_processed('delivery-1')
    assert q.is_duplicate('delivery-1') == True
    assert q.is_duplicate('delivery-2') == False

    print('Jobs OK')

asyncio.run(test())
"
```
  </verify>
  <done>JobQueue enqueues jobs, rejects duplicates, tracks delivery IDs for idempotency, and workers process jobs asynchronously with full state tracking.</done>
</task>

<task type="auto">
  <name>Task 2: Repository manager with fresh clone and cleanup</name>
  <files>src/booty/repositories.py</files>
  <action>
Create `src/booty/repositories.py` with:

**`prepare_workspace(job: Job, repo_url: str, branch: str, github_token: str = "") -> AsyncContextManager`**
An async context manager (use `contextlib.asynccontextmanager`) that:
1. Creates `tempfile.TemporaryDirectory(prefix=f"booty-{job.issue_number}-", ignore_cleanup_errors=True)`
2. Constructs clone URL: if github_token is provided and repo_url starts with "https://", inject token as `https://{token}@github.com/...`
3. Clones repo to temp dir using `git.Repo.clone_from(url, path, branch=branch)`. Run in executor (`asyncio.get_event_loop().run_in_executor(None, ...)`) since GitPython is synchronous/blocking.
4. Creates feature branch: `agent/issue-{job.issue_number}`
5. Checks out the feature branch
6. Yields a `Workspace` dataclass containing: `path: str`, `repo: git.Repo`, `branch: str`
7. On exit: calls `repo.close()` explicitly before temp dir cleanup
8. Logs: workspace_created, clone_complete, branch_created, workspace_cleaned events with job_id

**`Workspace` dataclass:**
- `path: str` (temp directory path)
- `repo: git.Repo`
- `branch: str` (feature branch name)

Use structlog logging with job_id bound to all messages.

IMPORTANT: Python imports must be at the top of the file. The clone operation is blocking I/O — it MUST run in an executor to not block the async event loop.
  </action>
  <verify>
Run (requires git):
```
python -c "
import asyncio
from booty.jobs import Job
from booty.repositories import prepare_workspace
import os

async def test():
    job = Job(job_id='test-1', issue_url='http://test', issue_number=42, payload={})
    # Use a small public repo for testing
    async with prepare_workspace(job, 'https://github.com/octocat/Hello-World.git', 'master') as ws:
        assert os.path.isdir(ws.path)
        assert os.path.isdir(os.path.join(ws.path, '.git'))
        assert ws.branch == 'agent/issue-42'
        print(f'Workspace at {ws.path}')
    # Verify cleanup
    print('Repos OK')

asyncio.run(test())
"
```
  </verify>
  <done>Repositories clone to isolated temp directories with feature branches. Cleanup happens automatically on context exit. Clone runs in executor (non-blocking).</done>
</task>

<task type="auto">
  <name>Task 3: Webhook endpoint and FastAPI app assembly</name>
  <files>src/booty/webhooks.py, src/booty/main.py</files>
  <action>
Create `src/booty/webhooks.py`:
- FastAPI `APIRouter` with prefix `/webhooks`
- Function `verify_signature(payload_body: bytes, secret: str, signature_header: str | None)` — raises HTTPException(401) if no signature header, HTTPException(403) if HMAC comparison fails. Uses `hmac.new()` with sha256 and `hmac.compare_digest()` for constant-time comparison.
- Route `POST /webhooks/github` that:
  1. Reads raw body with `await request.body()` BEFORE any JSON parsing (critical for HMAC)
  2. Gets headers: `X-Hub-Signature-256`, `X-GitHub-Delivery`, `X-GitHub-Event`
  3. Calls `verify_signature` with raw body
  4. Checks idempotency: if `X-GitHub-Delivery` already processed, return `{"status": "already_processed"}` with 200
  5. Filters: only process `X-GitHub-Event == "issues"` AND `payload["action"] == "labeled"` AND `payload["label"]["name"] == settings.TRIGGER_LABEL`. Return `{"status": "ignored"}` for non-matching events.
  6. Marks delivery as processed
  7. Creates Job with `job_id=f"{payload['issue']['number']}-{x_github_delivery}"`, enqueues it
  8. Returns `{"status": "accepted", "job_id": job.job_id}` with 202 status
- Log all decisions (signature valid, event filtered, job enqueued) with structlog

Create `src/booty/main.py`:
- Create `FastAPI(title="Booty", description="Self-managing builder agent")` app
- Add `CorrelationIdMiddleware` from asgi-correlation-id (must be added FIRST)
- Include webhook router
- Create module-level `job_queue = JobQueue(maxsize=settings.QUEUE_MAX_SIZE)`
- `@app.on_event("startup")` or lifespan context manager:
  1. Call `configure_logging(settings.LOG_LEVEL)`
  2. Start job queue workers: `await job_queue.start_workers(settings.WORKER_COUNT, process_job)`
  3. Log startup event
- `@app.on_event("shutdown")`:
  1. `await job_queue.shutdown()`
  2. Log shutdown event
- Define `async def process_job(job: Job)`:
  1. Bind job_id and issue_number to structlog context
  2. Log job_started
  3. Call `async with prepare_workspace(job, settings.TARGET_REPO_URL, settings.TARGET_BRANCH, settings.GITHUB_TOKEN) as workspace:`
  4. Log workspace_ready with workspace.path
  5. (Placeholder comment: "Phase 2 will add LLM code generation here")
  6. Log job_completed
- Add health check endpoint: `GET /health` returning `{"status": "ok"}`
- Make the job_queue accessible to webhooks module (pass via app.state or import from main)

Use the lifespan context manager pattern (preferred over deprecated on_event):
```python
from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    # startup
    configure_logging(settings.LOG_LEVEL)
    await job_queue.start_workers(settings.WORKER_COUNT, process_job)
    yield
    # shutdown
    await job_queue.shutdown()

app = FastAPI(lifespan=lifespan, ...)
```

Store job_queue on `app.state.job_queue` so webhooks.py can access it via `request.app.state.job_queue`.

IMPORTANT: Python imports must be at the top of each file.
  </action>
  <verify>
Run:
```
WEBHOOK_SECRET=test TARGET_REPO_URL=https://github.com/octocat/Hello-World.git python -c "
from booty.main import app
print('App created:', app.title)
print('Routes:', [r.path for r in app.routes])
"
```
Verify routes include `/webhooks/github` and `/health`.

Run the test suite:
```
WEBHOOK_SECRET=test TARGET_REPO_URL=https://github.com/octocat/Hello-World.git python -m pytest tests/ -v
```
(Tests will be minimal at this stage — focus on import verification and route existence.)
  </verify>
  <done>
FastAPI app starts with correlation ID middleware, webhook route validates HMAC signatures and filters events, jobs process asynchronously with fresh repo clones, and all operations produce structured JSON logs.
  </done>
</task>

</tasks>

<verification>
1. App starts: `WEBHOOK_SECRET=test TARGET_REPO_URL=https://github.com/test/repo uvicorn booty.main:app` runs without error
2. Health check works: `curl http://localhost:8000/health` returns `{"status": "ok"}`
3. Invalid signature rejected: POST to `/webhooks/github` without valid HMAC returns 401/403
4. Non-matching events ignored: POST with valid signature but wrong event type returns `{"status": "ignored"}`
5. Valid webhook accepted: POST with valid signature and matching label returns 202 with job_id
6. Duplicate delivery rejected: Same X-GitHub-Delivery header returns `{"status": "already_processed"}`
7. Logs are JSON with correlation IDs, job IDs, and timestamps
</verification>

<success_criteria>
- Webhook handler validates HMAC signatures and returns 200/202 within 2 seconds (REQ-01)
- Duplicate events produce no duplicate jobs via delivery ID tracking (REQ-02)
- Each job clones to an isolated temp directory with automatic cleanup (REQ-03)
- Repo URL, branch, label all come from environment config (REQ-04)
- Jobs process asynchronously via background workers (REQ-05)
- All logs are structured JSON with correlation IDs (REQ-06)
- All operational parameters are configurable with deterministic defaults (REQ-17)
</success_criteria>

<output>
After completion, create `.planning/phases/01-webhook-to-workspace-pipeline/01-02-SUMMARY.md`
</output>
