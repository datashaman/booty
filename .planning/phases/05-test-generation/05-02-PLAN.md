---
phase: 05-test-generation
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - src/booty/llm/models.py
  - src/booty/llm/prompts.py
  - src/booty/main.py
  - src/booty/code_gen/refiner.py
autonomous: true

must_haves:
  truths:
    - "Builder generates test files alongside source code in a single LLM call"
    - "Generated tests use the conventions detected from the target repository"
    - "Generated test imports are validated before tests are executed"
    - "Test files are included in the same commit as source changes"
    - "All generated tests pass before PR is finalized"
  artifacts:
    - path: "src/booty/llm/models.py"
      provides: "Extended CodeGenerationPlan with test_files field"
      contains: "test_files"
    - path: "src/booty/llm/prompts.py"
      provides: "Extended code generation prompt with test conventions injection"
      contains: "test_conventions"
    - path: "src/booty/main.py"
      provides: "Pipeline wiring: detection -> prompt injection -> validation"
      contains: "detect_conventions"
    - path: "src/booty/code_gen/refiner.py"
      provides: "Refinement loop forwarding test_conventions"
      contains: "test_conventions"
  key_links:
    - from: "src/booty/main.py"
      to: "src/booty/test_generation/detector.py"
      via: "calls detect_conventions on workspace"
      pattern: "detect_conventions.*workspace"
    - from: "src/booty/main.py"
      to: "src/booty/test_generation/validator.py"
      via: "validates test file imports after generation"
      pattern: "validate_test_imports"
    - from: "src/booty/llm/prompts.py"
      to: "src/booty/test_generation/models.py"
      via: "receives formatted conventions string in prompt"
      pattern: "test_conventions"
    - from: "src/booty/main.py"
      to: "src/booty/llm/prompts.py"
      via: "passes conventions to generate_code_changes"
      pattern: "generate_code_changes.*conventions"
    - from: "src/booty/code_gen/refiner.py"
      to: "src/booty/llm/prompts.py"
      via: "forwards test_conventions to regenerate_code_changes"
      pattern: "test_conventions"
---

<objective>
Wire convention detection and import validation into the LLM code generation pipeline so the Builder generates validated tests alongside source code.

Purpose: This completes the test generation feature by extending the LLM prompt to include test conventions, adding test_files tracking to the generation plan, and integrating detection + validation into the main issue-to-PR pipeline. After this plan, the Builder will produce tested PRs.

Output: Modified `llm/models.py`, `llm/prompts.py`, `code_gen/refiner.py`, and `main.py` with full test generation integration.
</objective>

<execution_context>
@/Users/marlinf/.claude/get-shit-done/workflows/execute-plan.md
@/Users/marlinf/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-test-generation/05-RESEARCH.md
@.planning/phases/05-test-generation/05-CONTEXT.md
@.planning/phases/05-test-generation/05-01-SUMMARY.md
@src/booty/llm/models.py
@src/booty/llm/prompts.py
@src/booty/main.py
@src/booty/code_gen/refiner.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend LLM models and prompts for test generation</name>
  <files>
    src/booty/llm/models.py
    src/booty/llm/prompts.py
  </files>
  <action>
**llm/models.py -- Extend CodeGenerationPlan:**

Add a `test_files` field to `CodeGenerationPlan`:
```python
test_files: list[FileChange] = Field(
    default_factory=list,
    description="Test file changes generated alongside source code"
)
```

Using `default_factory=list` ensures backward compatibility -- existing code that creates CodeGenerationPlan without test_files will still work (empty list default). The LLM will populate this when test conventions are injected into the prompt.

**llm/prompts.py -- Extend generate_code_changes:**

1. Add a `test_conventions: str = ""` parameter to the public `generate_code_changes()` function. This is the formatted string from `DetectedConventions.format_for_prompt()`.

2. Modify `_generate_code_changes_impl` prompt to include test generation instructions when `test_conventions` is non-empty. Add a new `{test_conventions}` template variable to the prompt. When empty string, the prompt behaves exactly as before (backward compatible).

3. Add these requirements to the prompt when test_conventions is present:
   - "Generate unit test files for all changed source files"
   - "Place test files in the `test_files` array, NOT in the `changes` array"
   - "Follow the repository conventions described above"
   - "Use ONLY imports that exist in the project dependencies -- DO NOT hallucinate package names"
   - "Each test file should cover happy path and basic edge cases"

4. Similarly extend `regenerate_code_changes` / `_regenerate_code_changes_impl` to accept and pass through `test_conventions`. During refinement, tests are NOT regenerated (per architectural decision: one-shot test generation, refine only code). But the prompt still needs test_conventions context so the LLM understands the test files exist and doesn't break them. Add a note in the regeneration prompt: "DO NOT modify test files. Only fix the source code to make existing tests pass."

Important: The `@prompt` decorator from magentic uses `{variable_name}` for template substitution. The `test_conventions` parameter must appear in the prompt template string. When it's empty string "", it effectively adds nothing to the prompt.
  </action>
  <verify>
    Run `python -c "from booty.llm.models import CodeGenerationPlan, FileChange; p = CodeGenerationPlan(changes=[], approach='test', testing_notes='test'); print(p.test_files)"` -- should print `[]`.
    Run `python -c "from booty.llm.models import CodeGenerationPlan, FileChange; p = CodeGenerationPlan(changes=[], approach='test', testing_notes='test', test_files=[FileChange(path='tests/test_x.py', content='pass', operation='create', explanation='test')]); print(len(p.test_files))"` -- should print `1`.
    Run `python -c "from booty.llm.prompts import generate_code_changes; import inspect; sig = inspect.signature(generate_code_changes); print('test_conventions' in sig.parameters)"` -- should print `True`.
  </verify>
  <done>
    CodeGenerationPlan has test_files field with empty list default.
    generate_code_changes accepts test_conventions parameter.
    regenerate_code_changes accepts test_conventions parameter.
    Prompt templates include test generation instructions when conventions are provided.
    Empty test_conventions preserves original behavior (backward compatible).
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire detection and validation into the issue-to-PR pipeline</name>
  <files>
    src/booty/main.py
    src/booty/code_gen/refiner.py
  </files>
  <action>
Modify `process_issue_to_pr()` in main.py to integrate test generation:

**Step 1a (after step 1, before step 2): Detect test conventions.**
After listing repo files and before analyzing the issue, run convention detection:
```python
from booty.test_generation import detect_conventions, validate_test_imports

conventions = detect_conventions(workspace_path)
logger.info(
    "test_conventions_detected",
    language=conventions.language,
    test_framework=conventions.test_framework,
    test_directory=conventions.test_directory,
)
test_conventions_text = conventions.format_for_prompt()
```

**Step 7 (code generation): Pass conventions to LLM.**
Update the `generate_code_changes` call to include `test_conventions=test_conventions_text`.

**Step 7b (new, after code generation): Validate test imports.**
After code generation, validate imports for each test file:
```python
if plan.test_files:
    logger.info("validating_test_imports", count=len(plan.test_files))
    for test_change in plan.test_files:
        if test_change.operation == "delete":
            continue
        is_valid, import_errors = validate_test_imports(
            test_change.content, conventions.language, workspace_path
        )
        if not is_valid:
            logger.warning(
                "test_import_validation_failed",
                path=test_change.path,
                errors=import_errors,
            )
            # Log warning but don't block -- tests will fail at execution and be caught by refinement
```
Log validation failures as warnings. Do NOT block the pipeline -- the test execution step will catch real failures, and the refinement loop handles fixes. Import validation is a safety signal, not a hard gate.

**Step 7c (new, after validation): Merge test files into changes for workspace apply.**
Combine `plan.changes` and `plan.test_files` into a single list for workspace application:
```python
all_changes = plan.changes + plan.test_files
```
Then use `all_changes` everywhere downstream that currently uses `plan.changes` for:
- Step 8: validate_generated_code loop
- Step 9: apply changes to workspace (modified_paths tracking)
- Step 10: pass to refine_until_tests_pass as current_changes

This ensures test files are written to the workspace before tests run, and are included in the commit.

**Step 10 (refinement): Pass conventions to refiner.**
Add `test_conventions: str = ""` parameter to `refine_until_tests_pass()` in `src/booty/code_gen/refiner.py`, and forward it to `regenerate_code_changes()` inside the refinement loop. Update the call in main.py to pass `test_conventions=test_conventions_text`.

**Important architectural notes:**
- Test files go in the SAME commit as source changes (per decision: atomic changes)
- During refinement, only source code is regenerated, not tests (per decision: one-shot test generation)
- The refiner's `regenerate_code_changes` prompt now tells the LLM "DO NOT modify test files"
- If test generation produces no test_files (empty list), pipeline works exactly as before
  </action>
  <verify>
    Run `python -c "from booty.main import process_issue_to_pr; import inspect; src = inspect.getsource(process_issue_to_pr); assert 'detect_conventions' in src; assert 'validate_test_imports' in src; assert 'test_files' in src; print('Pipeline wiring OK')"`.
    Run `python -c "from booty.code_gen.refiner import refine_until_tests_pass; import inspect; sig = inspect.signature(refine_until_tests_pass); assert 'test_conventions' in sig.parameters; print('Refiner updated OK')"`.
    Run `python -c "import ast; ast.parse(open('src/booty/main.py').read()); print('Syntax OK')"`.
  </verify>
  <done>
    Convention detection runs early in the pipeline and logs detected conventions.
    Test conventions are passed to LLM code generation prompt.
    Generated test file imports are validated (warnings logged for failures).
    Test files are merged into changes list for workspace application and commit.
    Test conventions are forwarded through the refinement loop.
    Refinement prompt instructs LLM not to modify test files.
    Pipeline is backward compatible -- works with repos that have no detectable conventions (test_files will be empty list).
  </done>
</task>

</tasks>

<verification>
- `python -c "import ast; ast.parse(open('src/booty/main.py').read()); ast.parse(open('src/booty/llm/prompts.py').read()); ast.parse(open('src/booty/llm/models.py').read()); ast.parse(open('src/booty/code_gen/refiner.py').read()); print('All syntax valid')"` passes
- `python -c "from booty.main import process_issue_to_pr; from booty.llm.models import CodeGenerationPlan; from booty.llm.prompts import generate_code_changes; print('All imports OK')"` passes
- CodeGenerationPlan has test_files field
- generate_code_changes accepts test_conventions
- process_issue_to_pr contains detect_conventions, validate_test_imports, and test_files handling
- refine_until_tests_pass accepts and forwards test_conventions
</verification>

<success_criteria>
- LLM prompt includes test generation instructions when conventions are detected
- Generated test imports are validated before execution
- Test files are written to workspace alongside source changes
- Test files are included in the same commit as source changes
- Refinement loop preserves test files (only regenerates source code)
- Pipeline works unchanged for repos with no detectable test conventions
</success_criteria>

<output>
After completion, create `.planning/phases/05-test-generation/05-02-SUMMARY.md`
</output>
