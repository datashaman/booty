---
phase: 10-import-compile-detection
plan: 03
type: execute
wave: 2
depends_on: ["01", "02"]
files_modified:
  - src/booty/verifier/runner.py
  - src/booty/github/checks.py
autonomous: true

must_haves:
  truths:
    - "Execution order: setup_command → install_command → import/compile sweep → test_command"
    - "Agent PR without install_command → fail with 'Config required for agent PRs: install_command.'"
    - "Import/compile failures produce check output with annotations (cap 50)"
    - "Title pattern: 'Verifier failed — Import errors' | 'Verifier failed — Compile errors' | 'Verifier failed — Multiple failure classes'"
  artifacts:
    - path: src/booty/verifier/runner.py
      provides: Full Phase 10 pipeline
      contains: "setup_command|install_command|compile_sweep|validate_imports"
    - path: src/booty/github/checks.py
      provides: edit_check_run with annotations support
      contains: "annotations"
  key_links:
    - from: runner.py
      to: verifier/imports.py
      via: compile_sweep, validate_imports
      pattern: "compile_sweep|validate_imports"
    - from: runner.py
      to: github/checks.py
      via: edit_check_run with output.annotations
      pattern: "edit_check_run"
---

<objective>
Wire import/compile detection into process_verifier_job. New execution order after clone: setup → install → import+compile sweep → tests. Extend checks.edit_check_run to pass annotations. Fail with formatted output on import/compile errors.

Purpose: VERIFY-11, VERIFY-12 — full Phase 10 integration.
Output: Runner with complete pipeline; check output with annotations.
</objective>

<execution_context>
@/Users/marlinf/.claude/get-shit-done/workflows/execute-plan.md
@/Users/marlinf/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/10-import-compile-detection/10-RESEARCH.md
@.planning/phases/10-import-compile-detection/10-CONTEXT.md
@src/booty/verifier/runner.py
@src/booty/github/checks.py
@src/booty/verifier/imports.py
@src/booty/verifier/limits.py
@src/booty/test_runner/executor.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend edit_check_run for annotations</name>
  <files>src/booty/github/checks.py</files>
  <action>
Ensure edit_check_run passes through output dict including annotations:

1. Current: edit_check_run(check_run, output={...}) passes output to check_run.edit()
2. GitHub API accepts output.annotations as array of {path, start_line, end_line, annotation_level, message, title}
3. No change needed if PyGithub passes dict through — verify. If output is transformed, ensure annotations preserved.
4. Add docstring note: output may include "annotations" array (max 50 per API); annotation_level one of notice, warning, failure.
</action>
  <verify>
cd /Users/marlinf/Projects/datashaman/booty && python -c "
from booty.github.checks import edit_check_run
import inspect
sig = inspect.signature(edit_check_run)
# output param should accept dict with arbitrary keys including annotations
src = inspect.getsource(edit_check_run)
assert 'output' in src
print('OK: edit_check_run accepts output')
"
  </verify>
  <done>edit_check_run passes output with annotations to check_run.edit().</done>
</task>

<task type="auto">
  <name>Task 2: Add execution order and agent PR install_command check</name>
  <files>src/booty/verifier/runner.py</files>
  <action>
After clone, after load_booty_config, before execute_tests:

1. If job.is_agent_pr and config is BootyConfigV1:
   - If getattr(config, 'install_command', None) in (None, ''):
     - edit_check_run(conclusion="failure", output={"title": "Verifier failed — Config required", "summary": "Config required for agent PRs: install_command."})
     - return

2. Run setup_command if present (BootyConfigV1.setup_command):
   - result = await execute_tests(config.setup_command, config.timeout, Path(workspace.path))
   - If result.exit_code != 0: parse_setup_stderr(result.stderr), build annotations, edit_check_run failure, return

3. Run install_command if present (BootyConfigV1.install_command):
   - result = await execute_tests(config.install_command, config.timeout, Path(workspace.path))
   - If result.exit_code != 0: edit_check_run failure (check-level; install failures not file-localized per CONTEXT), return

4. For BootyConfig (v0): no setup/install — skip to step 5.
</action>
  <verify>
cd /Users/marlinf/Projects/datashaman/booty && python -c "
from booty.verifier.runner import process_verifier_job
import inspect
src = inspect.getsource(process_verifier_job)
assert 'setup_command' in src or 'install_command' in src
print('OK: runner has setup/install path')
"
  </verify>
  <done>setup and install run before tests; agent PR without install_command fails.</done>
</task>

<task type="auto">
  <name>Task 3: Run import/compile sweep and wire failure reporting</name>
  <files>src/booty/verifier/runner.py</files>
  <action>
After install_command succeeds (or skipped for v0/non-agent):

1. Get changed .py files: repo = get_verifier_repo(...); stats = get_pr_diff_stats(repo, job.pr_number); py_files = [f.filename for f in stats.files if f.filename.endswith('.py')]. Need repo — use get_verifier_repo for job (works for any PR with installation_id).

2. If no py files: skip import/compile sweep, go to test_command.

3. For agent PR with install_command (or non-agent with install_command): run import+compile validation.
   For non-agent without install_command: CONTEXT says skip import validation; run compile + tests. So:
   - compile_sweep always (when py_files)
   - validate_imports only when install_command present

4. workspace_path = Path(workspace.path); file_paths = [workspace_path / f for f in py_files]
5. compile_errors = compile_sweep(file_paths, workspace_path)
6. import_errors = [] if not has_install else validate_imports(file_paths, workspace_path)
7. setup_annotations = parse_setup_stderr(result.stderr) if setup failed earlier — but we return on setup fail. So setup_annotations only when we have setup failure. Already handled in step 2 of Task 2.
8. All_annotations = compile_errors + import_errors
9. annotations, truncated = prepare_check_annotations(all_annotations, 50)
10. If annotations:
    - title = "Verifier failed — Import errors" | "Compile errors" | "Multiple failure classes" (based on which categories present)
    - summary = "N import errors, M compile errors. Tests not run." + (" Too many errors — showing first 50." if truncated else "")
    - edit_check_run(conclusion="failure", output={"title": title, "summary": summary, "annotations": annotations})
    - return
11. Else: proceed to execute_tests(config.test_command, ...)
</action>
  <verify>
cd /Users/marlinf/Projects/datashaman/booty && python -c "
from booty.verifier.runner import process_verifier_job
import inspect
src = inspect.getsource(process_verifier_job)
assert 'compile_sweep' in src or 'validate_imports' in src or 'imports' in src
print('OK: runner wires imports module')
"
  </verify>
  <done>Import and compile sweep run before tests; failures produce annotated check output.</done>
</task>

</tasks>

<verification>
- Full pipeline: schema → limits → clone → setup → install → import/compile → tests
- Annotations appear in check output on import/compile failure
- Non-agent without install_command: compile + tests; summary "Imports: skipped"
</verification>

<success_criteria>
- VERIFY-11: Unresolvable import → check failure with clear message and annotation
- VERIFY-12: setup_command compile failure or import sweep failure → check failure
- Failures reported in check output (title, summary, annotations where applicable)
</success_criteria>

<output>
After completion, create `.planning/phases/10-import-compile-detection/10-03-SUMMARY.md`
</output>
